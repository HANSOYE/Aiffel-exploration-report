{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af56eb47",
   "metadata": {},
   "source": [
    "# 인공지능 작사가 만들기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f74892c",
   "metadata": {},
   "source": [
    "exploration 4th basic을 하며 Cloud shell에 ~/aiffel/lyricist/data 생성한 덕에 ~/aiffel/lyricist/data/lyrics에 데이터가 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64bca4b9",
   "metadata": {},
   "source": [
    "# 1. 함수 preprocess_sentence()에서 문장 길이 조절하기\n",
    "\n",
    "\n",
    "1) 데이터 읽어오기\n",
    "\n",
    " : glob 모듈을 사용하면 파일을 읽어오는 작업을 하기가 아주 용이하다.\n",
    " \n",
    " : glob를 활용해 모든 txt파일을 읽어온 후, raw_corpus 리스트에 문장 단위로 저장해보자\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "80182002",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터 크기:  187088\n",
      "Examples :\n",
      " [\"Now I've heard there was a secret chord\", 'That David played, and it pleased the Lord', \"But you don't really care for music, do you?\"]\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import os\n",
    "import os, re\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "txt_file_path = os.getenv('HOME')+'/aiffel/lyricist/data/lyrics/*'\n",
    "\n",
    "txt_list = glob.glob(txt_file_path)\n",
    "\n",
    "#여러개의 txt 파일을 모두 읽어서 raw_corpus에 담아주기\n",
    "raw_corpus = []\n",
    "\n",
    "for txt_file in txt_list:\n",
    "    with open(txt_file, \"r\") as f:\n",
    "        raw = f.read().splitlines()\n",
    "        raw_corpus.extend(raw)\n",
    "        \n",
    "print(\"데이터 크기: \", len(raw_corpus))\n",
    "print(\"Examples :\\n\", raw_corpus[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a34b4ba5",
   "metadata": {},
   "source": [
    "2) 데이터 정제하기\n",
    "\n",
    " : preprocess_sentence() 함수 활용하기\n",
    " \n",
    " : 지나치게 긴 문장은 다른 데이터들이 과도한 padding을 갖게 하므로 제거한다.\n",
    " \n",
    " : 문장을 토큰화 했을 때 토큰의 개수가 15개를 넘어가는 문장을 학습 데이터에서 제외해보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b4e99fc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"Now I've heard there was a secret chord\", 'That David played, and it pleased the Lord', \"But you don't really care for music, do you?\", 'It goes like this', 'The fourth, the fifth', 'The minor fall, the major lift', 'The baffled king composing Hallelujah Hallelujah', 'Hallelujah', 'Hallelujah', 'Hallelujah Your faith was strong but you needed proof', 'You saw her bathing on the roof', 'Her beauty and the moonlight overthrew her', 'She tied you', 'To a kitchen chair', 'She broke your throne, and she cut your hair']\n"
     ]
    }
   ],
   "source": [
    "print(raw_corpus[:15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5a8f189d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#공백인 문장 지우기\n",
    "for idx, sentence in enumerate(raw_corpus):\n",
    "    if len(sentence) == 0: continue\n",
    "    if len(sentence.split()) >= 16: continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bbfc796c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<start> now i ve heard there was a secret chord <end>',\n",
       " '<start> that david played , and it pleased the lord <end>',\n",
       " '<start> but you don t really care for music , do you ? <end>',\n",
       " '<start> it goes like this <end>',\n",
       " '<start> the fourth , the fifth <end>',\n",
       " '<start> the minor fall , the major lift <end>',\n",
       " '<start> the baffled king composing hallelujah hallelujah <end>',\n",
       " '<start> hallelujah <end>',\n",
       " '<start> hallelujah <end>',\n",
       " '<start> hallelujah your faith was strong but you needed proof <end>']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#입력된 문장을\n",
    "# 1. 소문자로 바꾸고, 양쪽 공백을 지운다.\n",
    "# 2. 특수문자 양쪽에 공백을 넣고\n",
    "# 3. 여러개의 공백은 하나의 공백으로 바꾼다.\n",
    "# 4. a-zA-Z?.!,¿가 아닌 모든 문자를 하나의 공백으로 바꾼다.\n",
    "# 5. 다시 양쪽 공백을 지운다.\n",
    "# 6. 문장 시작에는 <start>, 끝에는 <end>를 추가한다.\n",
    "#이 순서로 처리해주면 문제가 되는 상황을 방지할 수 있을 것이다.\n",
    "def preprocess_sentence(sentence):\n",
    "    sentence = sentence.lower().strip() #1\n",
    "    sentence = re.sub(r\"([?.!,¿])\", r\" \\1 \", sentence) #2\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence) #3\n",
    "    sentence = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", sentence) #4\n",
    "    sentence = sentence.strip()\n",
    "    sentence = '<start> ' + sentence + ' <end>' #6\n",
    "    return sentence\n",
    "\n",
    "#정제 함수를 활용하여 정제된 문장 모으기\n",
    "corpus = []\n",
    "\n",
    "for sentence in raw_corpus:\n",
    "    #우리가 원하지 않는 문장은 건너뛴다.\n",
    "    if len(sentence) == 0: continue\n",
    "    if len(sentence.split()) >= 16: continue\n",
    "    \n",
    "    #정제하고 담기\n",
    "    preprocessed_sentence = preprocess_sentence(sentence)\n",
    "    corpus.append(preprocessed_sentence)\n",
    "    \n",
    "#정제된 결과를 10개 확인해보기\n",
    "corpus[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "596a35bb",
   "metadata": {},
   "source": [
    "3) 평가 데이터셋 분리\n",
    "\n",
    " : tokenize() 함수로 데이터를 Tensor로 변환한 후, sklearn 모듈의 train_test_split() 함수를 사용해 훈련 데이터와 평가 데이터를 분리하도록 하자.\n",
    " \n",
    "  : 단어장의 크기는 12,000 이상 으로 설정하자\n",
    "  \n",
    "  : 총 데이터의 20% 를 평가 데이터셋으로 사용하자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ee4e9dd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   2   50    5 ...    0    0    0]\n",
      " [   2   17 2714 ...    0    0    0]\n",
      " [   2   34    7 ...   44    3    0]\n",
      " ...\n",
      " [   2  259  193 ...   12    3    0]\n",
      " [   5   22    9 ...   10 1100    3]\n",
      " [   2    7   33 ...    0    0    0]] <keras_preprocessing.text.Tokenizer object at 0x7fca802d5f40>\n"
     ]
    }
   ],
   "source": [
    "def tokenize(corpus):\n",
    "    #7000단어를 기억할 수 있는 tokenizer를 만들것이다.\n",
    "    #우리는 이미 문장을 정제했으니 filters가 필요 없다.\n",
    "    #7000단어에 포함되지 못한 단어는 '<unk>'로 바꿀 것이다.\n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
    "        num_words = 14000,\n",
    "        filters = ' ',\n",
    "        oov_token = '<unk>')\n",
    "    \n",
    "    #corpus를 이용하여 tokenizer 내부의 단어장을 완성한다.\n",
    "    tokenizer.fit_on_texts(corpus)\n",
    "    #준비한 tokenizer를 이용해 corpus를 Tensor로 변환한다.\n",
    "    tensor = tokenizer.texts_to_sequences(corpus)\n",
    "    #입력 데이터의 시퀀스 길이를 일정하게 맞춰 준다.\n",
    "    #만약 시퀀스가 짧다면 문장 뒤에 패딩을 붙여 길이를 맞춰준다.\n",
    "    #문장 앞에 패딩을 붙여 길이를 맞추고 싶다면 padding='pre'를 사용한다.\n",
    "    \n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post', maxlen=15)\n",
    "    \n",
    "    print(tensor, tokenizer)\n",
    "    return tensor, tokenizer\n",
    "\n",
    "tensor, tokenizer = tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e4759bdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   2   50    5   91  307   62   57    9  957 5745]\n",
      " [   2   17 2714  879    4    8   11 6178    6  347]\n",
      " [   2   34    7   35   15  161  283   28  335    4]]\n"
     ]
    }
   ],
   "source": [
    "#생성된 텐서 데이터를 3번째 행, 10번째 열까지만 출력\n",
    "print(tensor[:3, :10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0c89e561",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 : <unk>\n",
      "2 : <start>\n",
      "3 : <end>\n",
      "4 : ,\n",
      "5 : i\n",
      "6 : the\n",
      "7 : you\n",
      "8 : and\n",
      "9 : a\n",
      "10 : to\n"
     ]
    }
   ],
   "source": [
    "#단어 사전 구축의 원리\n",
    "\n",
    "for idx in tokenizer.index_word:\n",
    "    print(idx, \":\", tokenizer.index_word[idx])\n",
    "    \n",
    "    if idx >= 10:break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f67662b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   2,   34, 1371, ...,    0,    0,    0],\n",
       "       [   2,   52,    5, ...,    0,    0,    0],\n",
       "       [   2,  100,  171, ...,    0,    0,    0],\n",
       "       ...,\n",
       "       [   2,    8, 1727, ...,    0,    0,    0],\n",
       "       [   2,    5,   61, ...,    0,    0,    0],\n",
       "       [   2,   34,    5, ...,    0,    0,    0]], dtype=int32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#생성된 텐서를 소스와 타겟으로 분리하여 모델이 학습할 수 있게 하자\n",
    "#tensor에서 마지막 토큰을 잘라내서 소스 문장을 생성한다.\n",
    "#마지막 토큰은 <end>가 아니라 <pad>일 가능성이 높다.\n",
    "src_input = tensor[:, :-1]\n",
    "#tensor에서 <start>를 잘라내서 타겟 문장을 생성한다.\n",
    "tgt_input = tensor[:, 1:]\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "enc_train, enc_val, dec_train, dec_val = train_test_split(src_input, tgt_input, test_size=0.2, random_state = 42)\n",
    "\n",
    "enc_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c59acd0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source Train: (134872, 14)\n",
      "Target Train: (134872, 14)\n"
     ]
    }
   ],
   "source": [
    "print(\"Source Train:\", enc_train.shape)\n",
    "print(\"Target Train:\", dec_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e8cd6fe",
   "metadata": {},
   "source": [
    "4) 인공지능 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a089ae11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((256, 14), (256, 14)), types: (tf.int32, tf.int32)>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BUFFER_SIZE = len(src_input)\n",
    "BATCH_SIZE = 256\n",
    "steps_per_epoch = len(src_input) // BATCH_SIZE\n",
    "\n",
    "#tokenizer가 구축한 단어사전 내 7000개와, 여기 포함되지 않은 0:<pad>를 포함하여 총 7001개이다.\n",
    "VOCAB_SIZE = tokenizer.num_words + 1\n",
    "\n",
    "#준비한 데이터 소스로부터 데이터셋을 만든다.\n",
    "#데이터셋에 대해서는 아래 문서를 참고하자\n",
    "#자세히 알아둘 수록 도움이 많이 되는 중요한 문서이다.\n",
    "# https://www.tensorflow.org/api_docs/python/tf/data/Dataset\n",
    "dataset = tf.data.Dataset.from_tensor_slices((src_input, tgt_input))\n",
    "dataset = dataset.shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder = True)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "13432050",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextGenerator(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_size, hidden_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_size)\n",
    "        self.rnn_1 = tf.keras.layers.LSTM(hidden_size, return_sequences = True)\n",
    "        self.rnn_2 = tf.keras.layers.LSTM(hidden_size, return_sequences = True)\n",
    "        self.linear = tf.keras.layers.Dense(vocab_size)\n",
    "        \n",
    "    def call(self, x):\n",
    "        out = self.embedding(x)\n",
    "        out = self.rnn_1(out)\n",
    "        out = self.rnn_2(out)\n",
    "        out = self.linear(out)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "embedding_size = 256 #워드 벡터의 차원수, 단어가 추상적으로 표현되는 크기이다.\n",
    "hidden_size = 1024 #주어진 동일한 데이터를 보고 각자 생각을 한 뒤 결정을 내리게 하는 '일꾼'의 수\n",
    "model = TextGenerator(tokenizer.num_words + 1, embedding_size, hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "014dc6ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(256, 14, 14001), dtype=float32, numpy=\n",
       "array([[[ 3.59002956e-06,  2.46070325e-04,  1.36797025e-04, ...,\n",
       "          1.76139918e-04,  1.33142850e-04, -1.31033457e-05],\n",
       "        [ 4.57876449e-05,  2.52802274e-04,  4.50991356e-04, ...,\n",
       "          1.96772045e-04,  2.02789688e-05, -1.08874716e-04],\n",
       "        [-1.65117701e-04,  1.46885548e-04,  7.58269220e-04, ...,\n",
       "          3.28453869e-04, -3.34363198e-04,  4.07478074e-05],\n",
       "        ...,\n",
       "        [-4.22220008e-04, -4.69377002e-04, -3.49098700e-04, ...,\n",
       "          1.25636265e-03, -4.27569603e-06,  2.20649221e-04],\n",
       "        [-3.42795043e-04, -5.89451985e-04, -2.02339710e-04, ...,\n",
       "          1.19847490e-03, -2.12579194e-04, -5.05906319e-05],\n",
       "        [-2.32568476e-04, -4.39667492e-04, -1.32197165e-04, ...,\n",
       "          1.11689209e-03, -2.66383460e-04, -3.61387589e-04]],\n",
       "\n",
       "       [[ 3.59002956e-06,  2.46070325e-04,  1.36797025e-04, ...,\n",
       "          1.76139918e-04,  1.33142850e-04, -1.31033457e-05],\n",
       "        [ 1.40509010e-05,  3.94782510e-05, -2.47495318e-05, ...,\n",
       "          5.33845101e-04,  1.06562053e-04,  2.73434242e-04],\n",
       "        [ 2.54722909e-05,  5.61112465e-05, -1.74486922e-05, ...,\n",
       "          6.98559510e-04,  1.75843335e-04,  3.10882693e-04],\n",
       "        ...,\n",
       "        [ 6.26421010e-04, -7.42674456e-04,  3.46956658e-04, ...,\n",
       "          6.17744809e-04,  8.91007308e-04,  1.28021545e-03],\n",
       "        [ 8.29549972e-04, -8.75883910e-04,  2.97802762e-04, ...,\n",
       "          5.77990082e-04,  1.06715283e-03,  1.60731131e-03],\n",
       "        [ 1.02168985e-03, -1.01007067e-03,  2.50342593e-04, ...,\n",
       "          5.37288433e-04,  1.19402923e-03,  1.91917794e-03]],\n",
       "\n",
       "       [[ 3.59002956e-06,  2.46070325e-04,  1.36797025e-04, ...,\n",
       "          1.76139918e-04,  1.33142850e-04, -1.31033457e-05],\n",
       "        [-7.83233281e-06,  2.02583891e-04,  4.01451747e-04, ...,\n",
       "          3.10548465e-04, -1.92684911e-05,  2.16678221e-04],\n",
       "        [-2.94264446e-05, -2.77299423e-05,  6.60395308e-04, ...,\n",
       "          7.95398373e-05, -1.64980418e-04,  1.14753857e-04],\n",
       "        ...,\n",
       "        [-8.25831085e-04,  6.04060711e-04,  2.89677206e-04, ...,\n",
       "         -1.84761753e-04, -1.17518369e-03, -1.48569793e-03],\n",
       "        [-7.29686464e-04,  4.75698442e-04,  5.41274960e-04, ...,\n",
       "          7.23964840e-05, -1.26127247e-03, -1.34138693e-03],\n",
       "        [-4.76192188e-04,  3.66530672e-04,  3.36199504e-04, ...,\n",
       "          2.78410589e-04, -1.31165353e-03, -8.50478187e-04]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[ 3.59002956e-06,  2.46070325e-04,  1.36797025e-04, ...,\n",
       "          1.76139918e-04,  1.33142850e-04, -1.31033457e-05],\n",
       "        [ 9.14377815e-05,  2.27549128e-04,  1.80749921e-04, ...,\n",
       "          1.72996180e-04,  2.07476827e-04, -4.82212599e-05],\n",
       "        [ 2.71748664e-04,  3.17986822e-04,  1.45258949e-04, ...,\n",
       "          2.37511384e-04,  9.91332217e-06,  7.12805850e-05],\n",
       "        ...,\n",
       "        [ 1.23875600e-03, -5.35008847e-04,  7.76673667e-04, ...,\n",
       "          4.93235129e-04,  1.09720067e-03,  1.39650912e-03],\n",
       "        [ 1.36952545e-03, -7.18536437e-04,  7.37636350e-04, ...,\n",
       "          4.76879562e-04,  1.22871529e-03,  1.73684559e-03],\n",
       "        [ 1.49110623e-03, -8.94576602e-04,  6.93559588e-04, ...,\n",
       "          4.44981240e-04,  1.32224569e-03,  2.05821544e-03]],\n",
       "\n",
       "       [[ 3.59002956e-06,  2.46070325e-04,  1.36797025e-04, ...,\n",
       "          1.76139918e-04,  1.33142850e-04, -1.31033457e-05],\n",
       "        [-1.44510486e-05,  3.07286886e-04,  3.78226599e-04, ...,\n",
       "          2.25261334e-04, -1.41471362e-04, -5.72402823e-05],\n",
       "        [ 1.55763031e-04, -5.24868665e-05,  7.35261769e-04, ...,\n",
       "          1.01831087e-04, -1.78837072e-04, -3.67705477e-04],\n",
       "        ...,\n",
       "        [ 5.49339049e-04,  2.05541524e-04,  1.68717757e-03, ...,\n",
       "         -3.06309928e-04, -3.09624302e-04,  3.21416708e-04],\n",
       "        [ 5.80381835e-04,  4.28967614e-05,  1.75487134e-03, ...,\n",
       "         -2.77068495e-04,  3.55607735e-05,  4.57227317e-04],\n",
       "        [ 6.46724249e-04, -1.29397275e-04,  1.72825926e-03, ...,\n",
       "         -2.26424076e-04,  4.11827088e-04,  6.79338060e-04]],\n",
       "\n",
       "       [[ 3.59002956e-06,  2.46070325e-04,  1.36797025e-04, ...,\n",
       "          1.76139918e-04,  1.33142850e-04, -1.31033457e-05],\n",
       "        [ 3.76101409e-04,  3.90425965e-04,  4.03545244e-04, ...,\n",
       "          3.12821794e-04,  1.14589078e-04, -5.98540682e-05],\n",
       "        [ 6.36388257e-04,  5.57426887e-04,  2.67973781e-04, ...,\n",
       "          3.93759809e-04,  1.17538642e-04, -2.09098816e-05],\n",
       "        ...,\n",
       "        [ 1.30354182e-03, -9.60268080e-04,  7.35285634e-04, ...,\n",
       "          4.20824013e-04,  1.30809459e-03,  2.37568305e-03],\n",
       "        [ 1.40157680e-03, -1.12280506e-03,  6.71847956e-04, ...,\n",
       "          3.72714421e-04,  1.35226233e-03,  2.64682062e-03],\n",
       "        [ 1.49186235e-03, -1.26784004e-03,  6.18738588e-04, ...,\n",
       "          3.21815867e-04,  1.37539872e-03,  2.88748695e-03]]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#데이터셋에서 데이터 한 배치만 불러오는 방법\n",
    "for src_sample, tgt_sample in dataset.take(1): break\n",
    "    \n",
    "#한 배치만 불러온 데이터를 모델에 넣어보자\n",
    "model(src_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d83d8b74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"text_generator\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        multiple                  3584256   \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  multiple                  5246976   \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                multiple                  8392704   \n",
      "_________________________________________________________________\n",
      "dense (Dense)                multiple                  14351025  \n",
      "=================================================================\n",
      "Total params: 31,574,961\n",
      "Trainable params: 31,574,961\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "570cd835",
   "metadata": {},
   "source": [
    "5) 훈련하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "46e2773c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "658/658 [==============================] - 123s 184ms/step - loss: 3.5709\n",
      "Epoch 2/10\n",
      "658/658 [==============================] - 120s 182ms/step - loss: 3.1010\n",
      "Epoch 3/10\n",
      "658/658 [==============================] - 120s 183ms/step - loss: 2.9155\n",
      "Epoch 4/10\n",
      "658/658 [==============================] - 120s 182ms/step - loss: 2.7740\n",
      "Epoch 5/10\n",
      "658/658 [==============================] - 120s 182ms/step - loss: 2.6509\n",
      "Epoch 6/10\n",
      "658/658 [==============================] - 120s 182ms/step - loss: 2.5399\n",
      "Epoch 7/10\n",
      "658/658 [==============================] - 120s 182ms/step - loss: 2.4365\n",
      "Epoch 8/10\n",
      "658/658 [==============================] - 120s 182ms/step - loss: 2.3414\n",
      "Epoch 9/10\n",
      "658/658 [==============================] - 120s 182ms/step - loss: 2.2511\n",
      "Epoch 10/10\n",
      "658/658 [==============================] - 120s 182ms/step - loss: 2.1666\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fc9d818a430>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "\n",
    "model.compile(loss=loss, optimizer = optimizer)\n",
    "model.fit(dataset, epochs = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b7a7e819",
   "metadata": {},
   "outputs": [],
   "source": [
    "#밑에서 만드는 generate_text 함수는 모델에게 시작 문장을 전달하면 모델이 시작 문장을 바탕으로 작문을 진행하게 한다.\n",
    "def generate_text(model, tokenizer, init_sentence = \"<start>\", max_len=20):\n",
    "    #테스트를 위해서 입력받은 init_sentence도 텐서로 변환한다.\n",
    "    test_input = tokenizer.texts_to_sequences([init_sentence])\n",
    "    test_tensor = tf.convert_to_tensor(test_input, dtype=tf.int64)\n",
    "    end_token = tokenizer.word_index[\"<end>\"]\n",
    "    \n",
    "    #단어 하나씩 예측해 문장을 만든다\n",
    "    # 1. 입력받은 문장의 텐서를 입력한다\n",
    "    # 2. 예측된 값 중 가장 높은 확률인 word index를 뽑아낸다.\n",
    "    # 3. 2에서 예측된 word index를 문장 뒤에 붙인다.\n",
    "    # 4. 모델이 <end>를 예측했거나, max_len에 도달했다면 문장 생성을 마친다.\n",
    "    while True:\n",
    "        #1\n",
    "        predict = model(test_tensor)\n",
    "        #2\n",
    "        predict_word = tf.argmax(tf.nn.softmax(predict, axis = -1), axis = -1)[:,-1]\n",
    "        #3\n",
    "        test_tensor = tf.concat([test_tensor, tf.expand_dims(predict_word, axis = 0)], axis=-1)\n",
    "        #4\n",
    "        if predict_word.numpy()[0] == end_token: break\n",
    "        if test_tensor.shape[1] >= max_len: break\n",
    "                \n",
    "    generated = \"\"\n",
    "    # tokenizer를 이용해 word index를 단어로 하나씩 변환한다.\n",
    "    for word_index in test_tensor[0].numpy():\n",
    "        generated += tokenizer.index_word[word_index] + \" \"\n",
    "        \n",
    "    return generated"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53312193",
   "metadata": {},
   "source": [
    "6) 결과물"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "55bcee42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> i love you , i m better , i m better <end> '"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> i love\", max_len=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4e16414c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> i do not like them <end> '"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> i do\", max_len=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2bf9af4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> christmas is the only one that i ve got <end> '"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> christmas is\", max_len=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e4eb80bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> baby , baby , baby , baby , baby , baby <end> '"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> baby\", max_len=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2b671afa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> merry christmas <end> '"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> merry\", max_len=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "535f565e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> now i m a survivor <end> '"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> now\", max_len=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3f6753d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> see i m a bad bitch <end> '"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> see\", max_len=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de42fca5",
   "metadata": {},
   "source": [
    "# 2. 함수 tokenize()에서 토큰 개수 조절하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2f17db46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터 크기:  187088\n",
      "Examples :\n",
      " [\"Now I've heard there was a secret chord\", 'That David played, and it pleased the Lord', \"But you don't really care for music, do you?\"]\n"
     ]
    }
   ],
   "source": [
    "txt_file_path = os.getenv('HOME')+'/aiffel/lyricist/data/lyrics/*'\n",
    "\n",
    "txt_list = glob.glob(txt_file_path)\n",
    "\n",
    "#여러개의 txt 파일을 모두 읽어서 raw_corpus에 담아주기\n",
    "raw_corpus = []\n",
    "\n",
    "for txt_file in txt_list:\n",
    "    with open(txt_file, \"r\") as f:\n",
    "        raw = f.read().splitlines()\n",
    "        raw_corpus.extend(raw)\n",
    "        \n",
    "print(\"데이터 크기: \", len(raw_corpus))\n",
    "print(\"Examples :\\n\", raw_corpus[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2476a845",
   "metadata": {},
   "outputs": [],
   "source": [
    "#공백인 문장 지우기\n",
    "for idx, sentence in enumerate(raw_corpus):\n",
    "    if len(sentence) == 0: continue\n",
    "        \n",
    "def preprocess_sentence(sentence):\n",
    "    sentence = sentence.lower().strip() #1\n",
    "    sentence = re.sub(r\"([?.!,¿])\", r\" \\1 \", sentence) #2\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence) #3\n",
    "    sentence = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", sentence) #4\n",
    "    sentence = sentence.strip()\n",
    "    sentence = '<start> ' + sentence + ' <end>' #6\n",
    "    return sentence\n",
    "\n",
    "#정제 함수를 활용하여 정제된 문장 모으기\n",
    "corpus = []\n",
    "\n",
    "for sentence in raw_corpus:\n",
    "    #우리가 원하지 않는 문장은 건너뛴다.\n",
    "    if len(sentence) == 0: continue\n",
    "    \n",
    "    #정제하고 담기\n",
    "    preprocessed_sentence = preprocess_sentence(sentence)\n",
    "    corpus.append(preprocessed_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "734a47c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   2   50    5 ...    0    0    0]\n",
      " [   2   17 2643 ...    0    0    0]\n",
      " [   2   35    7 ...   43    3    0]\n",
      " ...\n",
      " [   2    5  107 ...    0    0    0]\n",
      " [   2  261  200 ...   12    3    0]\n",
      " [   2    7   34 ...    0    0    0]] <keras_preprocessing.text.Tokenizer object at 0x7fc953fd4af0>\n"
     ]
    }
   ],
   "source": [
    "def tokenize(corpus):\n",
    "    #7000단어를 기억할 수 있는 tokenizer를 만들것이다.\n",
    "    #우리는 이미 문장을 정제했으니 filters가 필요 없다.\n",
    "    #7000단어에 포함되지 못한 단어는 '<unk>'로 바꿀 것이다.\n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
    "        num_words = 14000,\n",
    "        filters = ' ',\n",
    "        oov_token = '<unk>')\n",
    "    \n",
    "    #corpus를 이용하여 tokenizer 내부의 단어장을 완성한다.\n",
    "    tokenizer.fit_on_texts(corpus)\n",
    "    #준비한 tokenizer를 이용해 corpus를 Tensor로 변환한다.\n",
    "    tensor = tokenizer.texts_to_sequences(corpus)\n",
    "    \n",
    "    tensor = [x for x in tensor if len(x) <= 15]\n",
    "    #입력 데이터의 시퀀스 길이를 일정하게 맞춰 준다.\n",
    "    #만약 시퀀스가 짧다면 문장 뒤에 패딩을 붙여 길이를 맞춰준다.\n",
    "    #문장 앞에 패딩을 붙여 길이를 맞추고 싶다면 padding='pre'를 사용한다.\n",
    "    \n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post', maxlen=15)\n",
    "    \n",
    "    print(tensor, tokenizer)\n",
    "    return tensor, tokenizer\n",
    "\n",
    "tensor, tokenizer = tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7b4b35dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   2,    8,    7, ...,    3,    0,    0],\n",
       "       [   2,    5,   91, ...,    0,    0,    0],\n",
       "       [   2,  106,  353, ...,    0,    0,    0],\n",
       "       ...,\n",
       "       [   2,   12,   68, ...,    0,    0,    0],\n",
       "       [   2,  312,   23, ...,    0,    0,    0],\n",
       "       [   2, 2165,    5, ...,    1,    3,    0]], dtype=int32)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_input = tensor[:, :-1]\n",
    "#tensor에서 <start>를 잘라내서 타겟 문장을 생성한다.\n",
    "tgt_input = tensor[:, 1:]\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "enc_train, enc_val, dec_train, dec_val = train_test_split(src_input, tgt_input, test_size=0.2, random_state = 42)\n",
    "enc_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c914be89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source Train: (124981, 14)\n",
      "Target Train: (124981, 14)\n"
     ]
    }
   ],
   "source": [
    "print(\"Source Train:\", enc_train.shape)\n",
    "print(\"Target Train:\", dec_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d53874ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((256, 14), (256, 14)), types: (tf.int32, tf.int32)>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BUFFER_SIZE = len(src_input)\n",
    "BATCH_SIZE = 256\n",
    "steps_per_epoch = len(src_input) // BATCH_SIZE\n",
    "\n",
    "#tokenizer가 구축한 단어사전 내 7000개와, 여기 포함되지 않은 0:<pad>를 포함하여 총 7001개이다.\n",
    "VOCAB_SIZE = tokenizer.num_words + 1\n",
    "\n",
    "#준비한 데이터 소스로부터 데이터셋을 만든다.\n",
    "#데이터셋에 대해서는 아래 문서를 참고하자\n",
    "#자세히 알아둘 수록 도움이 많이 되는 중요한 문서이다.\n",
    "# https://www.tensorflow.org/api_docs/python/tf/data/Dataset\n",
    "dataset = tf.data.Dataset.from_tensor_slices((src_input, tgt_input))\n",
    "dataset = dataset.shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder = True)\n",
    "dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "329cc287",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextGenerator(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_size, hidden_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_size)\n",
    "        self.rnn_1 = tf.keras.layers.LSTM(hidden_size, return_sequences = True)\n",
    "        self.rnn_2 = tf.keras.layers.LSTM(hidden_size, return_sequences = True)\n",
    "        self.linear = tf.keras.layers.Dense(vocab_size)\n",
    "        \n",
    "    def call(self, x):\n",
    "        out = self.embedding(x)\n",
    "        out = self.rnn_1(out)\n",
    "        out = self.rnn_2(out)\n",
    "        out = self.linear(out)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "embedding_size = 256 #워드 벡터의 차원수, 단어가 추상적으로 표현되는 크기이다.\n",
    "hidden_size = 1024 #주어진 동일한 데이터를 보고 각자 생각을 한 뒤 결정을 내리게 하는 '일꾼'의 수\n",
    "model = TextGenerator(tokenizer.num_words + 1, embedding_size, hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8168ba34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "610/610 [==============================] - 116s 183ms/step - loss: 3.4376\n",
      "Epoch 2/10\n",
      "610/610 [==============================] - 112s 183ms/step - loss: 2.9795\n",
      "Epoch 3/10\n",
      "610/610 [==============================] - 111s 182ms/step - loss: 2.8136\n",
      "Epoch 4/10\n",
      "610/610 [==============================] - 111s 182ms/step - loss: 2.6872\n",
      "Epoch 5/10\n",
      "610/610 [==============================] - 111s 182ms/step - loss: 2.5772\n",
      "Epoch 6/10\n",
      "610/610 [==============================] - 111s 182ms/step - loss: 2.4788\n",
      "Epoch 7/10\n",
      "610/610 [==============================] - 111s 182ms/step - loss: 2.3869\n",
      "Epoch 8/10\n",
      "610/610 [==============================] - 112s 182ms/step - loss: 2.3011\n",
      "Epoch 9/10\n",
      "610/610 [==============================] - 111s 182ms/step - loss: 2.2201\n",
      "Epoch 10/10\n",
      "610/610 [==============================] - 111s 182ms/step - loss: 2.1440\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fc910674a90>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "\n",
    "model.compile(loss=loss, optimizer = optimizer)\n",
    "model.fit(dataset, epochs = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9e06ad8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#밑에서 만드는 generate_text 함수는 모델에게 시작 문장을 전달하면 모델이 시작 문장을 바탕으로 작문을 진행하게 한다.\n",
    "def generate_text(model, tokenizer, init_sentence = \"<start>\", max_len=20):\n",
    "    #테스트를 위해서 입력받은 init_sentence도 텐서로 변환한다.\n",
    "    test_input = tokenizer.texts_to_sequences([init_sentence])\n",
    "    test_tensor = tf.convert_to_tensor(test_input, dtype=tf.int64)\n",
    "    end_token = tokenizer.word_index[\"<end>\"]\n",
    "    \n",
    "    #단어 하나씩 예측해 문장을 만든다\n",
    "    # 1. 입력받은 문장의 텐서를 입력한다\n",
    "    # 2. 예측된 값 중 가장 높은 확률인 word index를 뽑아낸다.\n",
    "    # 3. 2에서 예측된 word index를 문장 뒤에 붙인다.\n",
    "    # 4. 모델이 <end>를 예측했거나, max_len에 도달했다면 문장 생성을 마친다.\n",
    "    while True:\n",
    "        #1\n",
    "        predict = model(test_tensor)\n",
    "        #2\n",
    "        predict_word = tf.argmax(tf.nn.softmax(predict, axis = -1), axis = -1)[:,-1]\n",
    "        #3\n",
    "        test_tensor = tf.concat([test_tensor, tf.expand_dims(predict_word, axis = 0)], axis=-1)\n",
    "        #4\n",
    "        if predict_word.numpy()[0] == end_token: break\n",
    "        if test_tensor.shape[1] >= max_len: break\n",
    "                \n",
    "    generated = \"\"\n",
    "    # tokenizer를 이용해 word index를 단어로 하나씩 변환한다.\n",
    "    for word_index in test_tensor[0].numpy():\n",
    "        generated += tokenizer.index_word[word_index] + \" \"\n",
    "        \n",
    "    return generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f622bf85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> i love you , i m a mess <end> '"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> i love\", max_len=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fcc19a1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> i do not like them <end> '"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> i do\", max_len=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5fc20e3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> christmas is the moon <end> '"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> christmas is\", max_len=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "edd4d6cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> baby , baby , baby <end> '"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> baby\", max_len=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a35d9b5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> merry christmas <end> '"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> merry\", max_len=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "dbfb912e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> now i m a voodoo chile <end> '"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> now\", max_len=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5105c9f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> see i had to go back to the <unk> <end> '"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> see\", max_len=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebd2b817",
   "metadata": {},
   "source": [
    "# 3. embedding_size 와 hidden_size 값 바꿔보기\n",
    "\n",
    "1) embedding_size = 256, hidden_size = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ac8dc46e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터 크기:  187088\n",
      "Examples :\n",
      " [\"Now I've heard there was a secret chord\", 'That David played, and it pleased the Lord', \"But you don't really care for music, do you?\"]\n",
      "[[   2   50    5 ...    0    0    0]\n",
      " [   2   17 2643 ...    0    0    0]\n",
      " [   2   35    7 ...   43    3    0]\n",
      " ...\n",
      " [   2    5  107 ...    0    0    0]\n",
      " [   2  261  200 ...   12    3    0]\n",
      " [   2    7   34 ...    0    0    0]] <keras_preprocessing.text.Tokenizer object at 0x7fc91084a460>\n",
      "Epoch 1/10\n",
      "610/610 [==============================] - 51s 80ms/step - loss: 3.7359\n",
      "Epoch 2/10\n",
      "610/610 [==============================] - 50s 81ms/step - loss: 3.2434\n",
      "Epoch 3/10\n",
      "610/610 [==============================] - 49s 80ms/step - loss: 3.0230\n",
      "Epoch 4/10\n",
      "610/610 [==============================] - 50s 81ms/step - loss: 2.8878\n",
      "Epoch 5/10\n",
      "610/610 [==============================] - 50s 81ms/step - loss: 2.7866\n",
      "Epoch 6/10\n",
      "610/610 [==============================] - 50s 81ms/step - loss: 2.7004\n",
      "Epoch 7/10\n",
      "610/610 [==============================] - 50s 81ms/step - loss: 2.6228\n",
      "Epoch 8/10\n",
      "610/610 [==============================] - 50s 81ms/step - loss: 2.5505\n",
      "Epoch 9/10\n",
      "610/610 [==============================] - 50s 81ms/step - loss: 2.4810\n",
      "Epoch 10/10\n",
      "610/610 [==============================] - 50s 81ms/step - loss: 2.4145\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fc9ece49760>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt_file_path = os.getenv('HOME')+'/aiffel/lyricist/data/lyrics/*'\n",
    "\n",
    "txt_list = glob.glob(txt_file_path)\n",
    "\n",
    "raw_corpus = []\n",
    "\n",
    "for txt_file in txt_list:\n",
    "    with open(txt_file, \"r\") as f:\n",
    "        raw = f.read().splitlines()\n",
    "        raw_corpus.extend(raw)\n",
    "        \n",
    "print(\"데이터 크기: \", len(raw_corpus))\n",
    "print(\"Examples :\\n\", raw_corpus[:3])\n",
    "\n",
    "for idx, sentence in enumerate(raw_corpus):\n",
    "    if len(sentence) == 0: continue\n",
    "        \n",
    "def preprocess_sentence(sentence):\n",
    "    sentence = sentence.lower().strip() #1\n",
    "    sentence = re.sub(r\"([?.!,¿])\", r\" \\1 \", sentence) #2\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence) #3\n",
    "    sentence = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", sentence) #4\n",
    "    sentence = sentence.strip()\n",
    "    sentence = '<start> ' + sentence + ' <end>' #6\n",
    "    return sentence\n",
    "\n",
    "corpus = []\n",
    "\n",
    "for sentence in raw_corpus:\n",
    "    if len(sentence) == 0: continue\n",
    "       \n",
    "    preprocessed_sentence = preprocess_sentence(sentence)\n",
    "    corpus.append(preprocessed_sentence)\n",
    "    \n",
    "def tokenize(corpus):\n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
    "        num_words = 14000,\n",
    "        filters = ' ',\n",
    "        oov_token = '<unk>')\n",
    "    \n",
    "    tokenizer.fit_on_texts(corpus)\n",
    "    \n",
    "    tensor = tokenizer.texts_to_sequences(corpus)\n",
    "    \n",
    "    tensor = [x for x in tensor if len(x) <= 15]\n",
    "    \n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post', maxlen=15)\n",
    "    \n",
    "    print(tensor, tokenizer)\n",
    "    return tensor, tokenizer\n",
    "\n",
    "tensor, tokenizer = tokenize(corpus)\n",
    "\n",
    "src_input = tensor[:, :-1]\n",
    "\n",
    "tgt_input = tensor[:, 1:]\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "enc_train, enc_val, dec_train, dec_val = train_test_split(src_input, tgt_input, test_size=0.2, random_state = 42)\n",
    "\n",
    "BUFFER_SIZE = len(src_input)\n",
    "BATCH_SIZE = 256\n",
    "steps_per_epoch = len(src_input) // BATCH_SIZE\n",
    "\n",
    "VOCAB_SIZE = tokenizer.num_words + 1\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((src_input, tgt_input))\n",
    "dataset = dataset.shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder = True)\n",
    "\n",
    "class TextGenerator(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_size, hidden_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_size)\n",
    "        self.rnn_1 = tf.keras.layers.LSTM(hidden_size, return_sequences = True)\n",
    "        self.rnn_2 = tf.keras.layers.LSTM(hidden_size, return_sequences = True)\n",
    "        self.linear = tf.keras.layers.Dense(vocab_size)\n",
    "        \n",
    "    def call(self, x):\n",
    "        out = self.embedding(x)\n",
    "        out = self.rnn_1(out)\n",
    "        out = self.rnn_2(out)\n",
    "        out = self.linear(out)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "embedding_size = 256\n",
    "hidden_size = 512\n",
    "model = TextGenerator(tokenizer.num_words + 1, embedding_size, hidden_size)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "\n",
    "model.compile(loss=loss, optimizer = optimizer)\n",
    "model.fit(dataset, epochs = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "805c057d",
   "metadata": {},
   "source": [
    "2) embedding_size = 256, hidden_size = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0e7d7dc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터 크기:  187088\n",
      "Examples :\n",
      " [\"Now I've heard there was a secret chord\", 'That David played, and it pleased the Lord', \"But you don't really care for music, do you?\"]\n",
      "[[   2   50    5 ...    0    0    0]\n",
      " [   2   17 2643 ...    0    0    0]\n",
      " [   2   35    7 ...   43    3    0]\n",
      " ...\n",
      " [   2    5  107 ...    0    0    0]\n",
      " [   2  261  200 ...   12    3    0]\n",
      " [   2    7   34 ...    0    0    0]] <keras_preprocessing.text.Tokenizer object at 0x7fc910195af0>\n",
      "Epoch 1/10\n",
      "610/610 [==============================] - 28s 43ms/step - loss: 3.8797\n",
      "Epoch 2/10\n",
      "610/610 [==============================] - 27s 44ms/step - loss: 3.3908\n",
      "Epoch 3/10\n",
      "610/610 [==============================] - 26s 43ms/step - loss: 3.2212\n",
      "Epoch 4/10\n",
      "610/610 [==============================] - 27s 43ms/step - loss: 3.0722\n",
      "Epoch 5/10\n",
      "610/610 [==============================] - 27s 44ms/step - loss: 2.9884\n",
      "Epoch 6/10\n",
      "610/610 [==============================] - 27s 44ms/step - loss: 2.9224\n",
      "Epoch 7/10\n",
      "610/610 [==============================] - 27s 44ms/step - loss: 2.8584\n",
      "Epoch 8/10\n",
      "610/610 [==============================] - 27s 44ms/step - loss: 2.7941\n",
      "Epoch 9/10\n",
      "610/610 [==============================] - 27s 44ms/step - loss: 2.7302\n",
      "Epoch 10/10\n",
      "610/610 [==============================] - 27s 44ms/step - loss: 2.6682\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fc9eaadc790>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt_file_path = os.getenv('HOME')+'/aiffel/lyricist/data/lyrics/*'\n",
    "\n",
    "txt_list = glob.glob(txt_file_path)\n",
    "\n",
    "raw_corpus = []\n",
    "\n",
    "for txt_file in txt_list:\n",
    "    with open(txt_file, \"r\") as f:\n",
    "        raw = f.read().splitlines()\n",
    "        raw_corpus.extend(raw)\n",
    "        \n",
    "print(\"데이터 크기: \", len(raw_corpus))\n",
    "print(\"Examples :\\n\", raw_corpus[:3])\n",
    "\n",
    "for idx, sentence in enumerate(raw_corpus):\n",
    "    if len(sentence) == 0: continue\n",
    "        \n",
    "def preprocess_sentence(sentence):\n",
    "    sentence = sentence.lower().strip() #1\n",
    "    sentence = re.sub(r\"([?.!,¿])\", r\" \\1 \", sentence) #2\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence) #3\n",
    "    sentence = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", sentence) #4\n",
    "    sentence = sentence.strip()\n",
    "    sentence = '<start> ' + sentence + ' <end>' #6\n",
    "    return sentence\n",
    "\n",
    "corpus = []\n",
    "\n",
    "for sentence in raw_corpus:\n",
    "    if len(sentence) == 0: continue\n",
    "       \n",
    "    preprocessed_sentence = preprocess_sentence(sentence)\n",
    "    corpus.append(preprocessed_sentence)\n",
    "    \n",
    "def tokenize(corpus):\n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
    "        num_words = 14000,\n",
    "        filters = ' ',\n",
    "        oov_token = '<unk>')\n",
    "    \n",
    "    tokenizer.fit_on_texts(corpus)\n",
    "    \n",
    "    tensor = tokenizer.texts_to_sequences(corpus)\n",
    "    \n",
    "    tensor = [x for x in tensor if len(x) <= 15]\n",
    "    \n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post', maxlen=15)\n",
    "    \n",
    "    print(tensor, tokenizer)\n",
    "    return tensor, tokenizer\n",
    "\n",
    "tensor, tokenizer = tokenize(corpus)\n",
    "\n",
    "src_input = tensor[:, :-1]\n",
    "\n",
    "tgt_input = tensor[:, 1:]\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "enc_train, enc_val, dec_train, dec_val = train_test_split(src_input, tgt_input, test_size=0.2, random_state = 42)\n",
    "\n",
    "BUFFER_SIZE = len(src_input)\n",
    "BATCH_SIZE = 256\n",
    "steps_per_epoch = len(src_input) // BATCH_SIZE\n",
    "\n",
    "VOCAB_SIZE = tokenizer.num_words + 1\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((src_input, tgt_input))\n",
    "dataset = dataset.shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder = True)\n",
    "\n",
    "class TextGenerator(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_size, hidden_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_size)\n",
    "        self.rnn_1 = tf.keras.layers.LSTM(hidden_size, return_sequences = True)\n",
    "        self.rnn_2 = tf.keras.layers.LSTM(hidden_size, return_sequences = True)\n",
    "        self.linear = tf.keras.layers.Dense(vocab_size)\n",
    "        \n",
    "    def call(self, x):\n",
    "        out = self.embedding(x)\n",
    "        out = self.rnn_1(out)\n",
    "        out = self.rnn_2(out)\n",
    "        out = self.linear(out)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "embedding_size = 256\n",
    "hidden_size = 256\n",
    "model = TextGenerator(tokenizer.num_words + 1, embedding_size, hidden_size)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "\n",
    "model.compile(loss=loss, optimizer = optimizer)\n",
    "model.fit(dataset, epochs = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c7bc29c",
   "metadata": {},
   "source": [
    "3) embedding_size = 256, hidden_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "680814cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터 크기:  187088\n",
      "Examples :\n",
      " [\"Now I've heard there was a secret chord\", 'That David played, and it pleased the Lord', \"But you don't really care for music, do you?\"]\n",
      "[[   2   50    5 ...    0    0    0]\n",
      " [   2   17 2643 ...    0    0    0]\n",
      " [   2   35    7 ...   43    3    0]\n",
      " ...\n",
      " [   2    5  107 ...    0    0    0]\n",
      " [   2  261  200 ...   12    3    0]\n",
      " [   2    7   34 ...    0    0    0]] <keras_preprocessing.text.Tokenizer object at 0x7fc9101d1670>\n",
      "Epoch 1/10\n",
      "610/610 [==============================] - 19s 28ms/step - loss: 4.1071\n",
      "Epoch 2/10\n",
      "610/610 [==============================] - 18s 29ms/step - loss: 3.4731\n",
      "Epoch 3/10\n",
      "610/610 [==============================] - 18s 29ms/step - loss: 3.3645\n",
      "Epoch 4/10\n",
      "610/610 [==============================] - 17s 28ms/step - loss: 3.2579\n",
      "Epoch 5/10\n",
      "610/610 [==============================] - 17s 28ms/step - loss: 3.1547\n",
      "Epoch 6/10\n",
      "610/610 [==============================] - 18s 29ms/step - loss: 3.0578\n",
      "Epoch 7/10\n",
      "610/610 [==============================] - 18s 29ms/step - loss: 2.9836\n",
      "Epoch 8/10\n",
      "610/610 [==============================] - 18s 29ms/step - loss: 2.9214\n",
      "Epoch 9/10\n",
      "610/610 [==============================] - 18s 29ms/step - loss: 2.8652\n",
      "Epoch 10/10\n",
      "610/610 [==============================] - 18s 29ms/step - loss: 2.8133\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fc951cfcbb0>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt_file_path = os.getenv('HOME')+'/aiffel/lyricist/data/lyrics/*'\n",
    "\n",
    "txt_list = glob.glob(txt_file_path)\n",
    "\n",
    "raw_corpus = []\n",
    "\n",
    "for txt_file in txt_list:\n",
    "    with open(txt_file, \"r\") as f:\n",
    "        raw = f.read().splitlines()\n",
    "        raw_corpus.extend(raw)\n",
    "        \n",
    "print(\"데이터 크기: \", len(raw_corpus))\n",
    "print(\"Examples :\\n\", raw_corpus[:3])\n",
    "\n",
    "for idx, sentence in enumerate(raw_corpus):\n",
    "    if len(sentence) == 0: continue\n",
    "        \n",
    "def preprocess_sentence(sentence):\n",
    "    sentence = sentence.lower().strip() #1\n",
    "    sentence = re.sub(r\"([?.!,¿])\", r\" \\1 \", sentence) #2\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence) #3\n",
    "    sentence = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", sentence) #4\n",
    "    sentence = sentence.strip()\n",
    "    sentence = '<start> ' + sentence + ' <end>' #6\n",
    "    return sentence\n",
    "\n",
    "corpus = []\n",
    "\n",
    "for sentence in raw_corpus:\n",
    "    if len(sentence) == 0: continue\n",
    "       \n",
    "    preprocessed_sentence = preprocess_sentence(sentence)\n",
    "    corpus.append(preprocessed_sentence)\n",
    "    \n",
    "def tokenize(corpus):\n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
    "        num_words = 14000,\n",
    "        filters = ' ',\n",
    "        oov_token = '<unk>')\n",
    "    \n",
    "    tokenizer.fit_on_texts(corpus)\n",
    "    \n",
    "    tensor = tokenizer.texts_to_sequences(corpus)\n",
    "    \n",
    "    tensor = [x for x in tensor if len(x) <= 15]\n",
    "    \n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post', maxlen=15)\n",
    "    \n",
    "    print(tensor, tokenizer)\n",
    "    return tensor, tokenizer\n",
    "\n",
    "tensor, tokenizer = tokenize(corpus)\n",
    "\n",
    "src_input = tensor[:, :-1]\n",
    "\n",
    "tgt_input = tensor[:, 1:]\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "enc_train, enc_val, dec_train, dec_val = train_test_split(src_input, tgt_input, test_size=0.2, random_state = 42)\n",
    "\n",
    "BUFFER_SIZE = len(src_input)\n",
    "BATCH_SIZE = 256\n",
    "steps_per_epoch = len(src_input) // BATCH_SIZE\n",
    "\n",
    "VOCAB_SIZE = tokenizer.num_words + 1\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((src_input, tgt_input))\n",
    "dataset = dataset.shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder = True)\n",
    "\n",
    "class TextGenerator(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_size, hidden_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_size)\n",
    "        self.rnn_1 = tf.keras.layers.LSTM(hidden_size, return_sequences = True)\n",
    "        self.rnn_2 = tf.keras.layers.LSTM(hidden_size, return_sequences = True)\n",
    "        self.linear = tf.keras.layers.Dense(vocab_size)\n",
    "        \n",
    "    def call(self, x):\n",
    "        out = self.embedding(x)\n",
    "        out = self.rnn_1(out)\n",
    "        out = self.rnn_2(out)\n",
    "        out = self.linear(out)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "embedding_size = 256\n",
    "hidden_size = 128\n",
    "model = TextGenerator(tokenizer.num_words + 1, embedding_size, hidden_size)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "\n",
    "model.compile(loss=loss, optimizer = optimizer)\n",
    "model.fit(dataset, epochs = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a313ff0b",
   "metadata": {},
   "source": [
    "4) embedding_size = 128, hidden_size = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "710ce5a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터 크기:  187088\n",
      "Examples :\n",
      " [\"Now I've heard there was a secret chord\", 'That David played, and it pleased the Lord', \"But you don't really care for music, do you?\"]\n",
      "[[   2   50    5 ...    0    0    0]\n",
      " [   2   17 2643 ...    0    0    0]\n",
      " [   2   35    7 ...   43    3    0]\n",
      " ...\n",
      " [   2    5  107 ...    0    0    0]\n",
      " [   2  261  200 ...   12    3    0]\n",
      " [   2    7   34 ...    0    0    0]] <keras_preprocessing.text.Tokenizer object at 0x7fc91084a7c0>\n",
      "Epoch 1/10\n",
      "610/610 [==============================] - 115s 181ms/step - loss: 3.4776\n",
      "Epoch 2/10\n",
      "610/610 [==============================] - 111s 182ms/step - loss: 3.0322\n",
      "Epoch 3/10\n",
      "610/610 [==============================] - 111s 182ms/step - loss: 2.8635\n",
      "Epoch 4/10\n",
      "610/610 [==============================] - 111s 182ms/step - loss: 2.7386\n",
      "Epoch 5/10\n",
      "610/610 [==============================] - 111s 182ms/step - loss: 2.6344\n",
      "Epoch 6/10\n",
      "610/610 [==============================] - 111s 182ms/step - loss: 2.5428\n",
      "Epoch 7/10\n",
      "610/610 [==============================] - 111s 182ms/step - loss: 2.4591\n",
      "Epoch 8/10\n",
      "610/610 [==============================] - 111s 182ms/step - loss: 2.3821\n",
      "Epoch 9/10\n",
      "610/610 [==============================] - 111s 182ms/step - loss: 2.3097\n",
      "Epoch 10/10\n",
      "610/610 [==============================] - 111s 182ms/step - loss: 2.2420\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fc9ea8fa640>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt_file_path = os.getenv('HOME')+'/aiffel/lyricist/data/lyrics/*'\n",
    "\n",
    "txt_list = glob.glob(txt_file_path)\n",
    "\n",
    "raw_corpus = []\n",
    "\n",
    "for txt_file in txt_list:\n",
    "    with open(txt_file, \"r\") as f:\n",
    "        raw = f.read().splitlines()\n",
    "        raw_corpus.extend(raw)\n",
    "        \n",
    "print(\"데이터 크기: \", len(raw_corpus))\n",
    "print(\"Examples :\\n\", raw_corpus[:3])\n",
    "\n",
    "for idx, sentence in enumerate(raw_corpus):\n",
    "    if len(sentence) == 0: continue\n",
    "        \n",
    "def preprocess_sentence(sentence):\n",
    "    sentence = sentence.lower().strip() #1\n",
    "    sentence = re.sub(r\"([?.!,¿])\", r\" \\1 \", sentence) #2\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence) #3\n",
    "    sentence = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", sentence) #4\n",
    "    sentence = sentence.strip()\n",
    "    sentence = '<start> ' + sentence + ' <end>' #6\n",
    "    return sentence\n",
    "\n",
    "corpus = []\n",
    "\n",
    "for sentence in raw_corpus:\n",
    "    if len(sentence) == 0: continue\n",
    "       \n",
    "    preprocessed_sentence = preprocess_sentence(sentence)\n",
    "    corpus.append(preprocessed_sentence)\n",
    "    \n",
    "def tokenize(corpus):\n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
    "        num_words = 14000,\n",
    "        filters = ' ',\n",
    "        oov_token = '<unk>')\n",
    "    \n",
    "    tokenizer.fit_on_texts(corpus)\n",
    "    \n",
    "    tensor = tokenizer.texts_to_sequences(corpus)\n",
    "    \n",
    "    tensor = [x for x in tensor if len(x) <= 15]\n",
    "    \n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post', maxlen=15)\n",
    "    \n",
    "    print(tensor, tokenizer)\n",
    "    return tensor, tokenizer\n",
    "\n",
    "tensor, tokenizer = tokenize(corpus)\n",
    "\n",
    "src_input = tensor[:, :-1]\n",
    "\n",
    "tgt_input = tensor[:, 1:]\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "enc_train, enc_val, dec_train, dec_val = train_test_split(src_input, tgt_input, test_size=0.2, random_state = 42)\n",
    "\n",
    "BUFFER_SIZE = len(src_input)\n",
    "BATCH_SIZE = 256\n",
    "steps_per_epoch = len(src_input) // BATCH_SIZE\n",
    "\n",
    "VOCAB_SIZE = tokenizer.num_words + 1\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((src_input, tgt_input))\n",
    "dataset = dataset.shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder = True)\n",
    "\n",
    "class TextGenerator(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_size, hidden_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_size)\n",
    "        self.rnn_1 = tf.keras.layers.LSTM(hidden_size, return_sequences = True)\n",
    "        self.rnn_2 = tf.keras.layers.LSTM(hidden_size, return_sequences = True)\n",
    "        self.linear = tf.keras.layers.Dense(vocab_size)\n",
    "        \n",
    "    def call(self, x):\n",
    "        out = self.embedding(x)\n",
    "        out = self.rnn_1(out)\n",
    "        out = self.rnn_2(out)\n",
    "        out = self.linear(out)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "embedding_size = 128\n",
    "hidden_size = 1024\n",
    "model = TextGenerator(tokenizer.num_words + 1, embedding_size, hidden_size)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "\n",
    "model.compile(loss=loss, optimizer = optimizer)\n",
    "model.fit(dataset, epochs = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d8e6c31",
   "metadata": {},
   "source": [
    "5) embedding_size = 64, hidden_size = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "482712c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터 크기:  187088\n",
      "Examples :\n",
      " [\"Now I've heard there was a secret chord\", 'That David played, and it pleased the Lord', \"But you don't really care for music, do you?\"]\n",
      "[[   2   50    5 ...    0    0    0]\n",
      " [   2   17 2643 ...    0    0    0]\n",
      " [   2   35    7 ...   43    3    0]\n",
      " ...\n",
      " [   2    5  107 ...    0    0    0]\n",
      " [   2  261  200 ...   12    3    0]\n",
      " [   2    7   34 ...    0    0    0]] <keras_preprocessing.text.Tokenizer object at 0x7fc9d82b5ca0>\n",
      "Epoch 1/10\n",
      "610/610 [==============================] - 113s 178ms/step - loss: 3.5137\n",
      "Epoch 2/10\n",
      "610/610 [==============================] - 110s 180ms/step - loss: 3.0919\n",
      "Epoch 3/10\n",
      "610/610 [==============================] - 109s 179ms/step - loss: 2.9499\n",
      "Epoch 4/10\n",
      "610/610 [==============================] - 109s 179ms/step - loss: 2.8314\n",
      "Epoch 5/10\n",
      "610/610 [==============================] - 110s 180ms/step - loss: 2.7321\n",
      "Epoch 6/10\n",
      "610/610 [==============================] - 109s 179ms/step - loss: 2.6459\n",
      "Epoch 7/10\n",
      "610/610 [==============================] - 110s 180ms/step - loss: 2.5676\n",
      "Epoch 8/10\n",
      "610/610 [==============================] - 110s 181ms/step - loss: 2.4947\n",
      "Epoch 9/10\n",
      "610/610 [==============================] - 110s 179ms/step - loss: 2.4261\n",
      "Epoch 10/10\n",
      "610/610 [==============================] - 110s 180ms/step - loss: 2.3620\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fc9e29c43d0>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt_file_path = os.getenv('HOME')+'/aiffel/lyricist/data/lyrics/*'\n",
    "\n",
    "txt_list = glob.glob(txt_file_path)\n",
    "\n",
    "raw_corpus = []\n",
    "\n",
    "for txt_file in txt_list:\n",
    "    with open(txt_file, \"r\") as f:\n",
    "        raw = f.read().splitlines()\n",
    "        raw_corpus.extend(raw)\n",
    "        \n",
    "print(\"데이터 크기: \", len(raw_corpus))\n",
    "print(\"Examples :\\n\", raw_corpus[:3])\n",
    "\n",
    "for idx, sentence in enumerate(raw_corpus):\n",
    "    if len(sentence) == 0: continue\n",
    "        \n",
    "def preprocess_sentence(sentence):\n",
    "    sentence = sentence.lower().strip() #1\n",
    "    sentence = re.sub(r\"([?.!,¿])\", r\" \\1 \", sentence) #2\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence) #3\n",
    "    sentence = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", sentence) #4\n",
    "    sentence = sentence.strip()\n",
    "    sentence = '<start> ' + sentence + ' <end>' #6\n",
    "    return sentence\n",
    "\n",
    "corpus = []\n",
    "\n",
    "for sentence in raw_corpus:\n",
    "    if len(sentence) == 0: continue\n",
    "       \n",
    "    preprocessed_sentence = preprocess_sentence(sentence)\n",
    "    corpus.append(preprocessed_sentence)\n",
    "    \n",
    "def tokenize(corpus):\n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
    "        num_words = 14000,\n",
    "        filters = ' ',\n",
    "        oov_token = '<unk>')\n",
    "    \n",
    "    tokenizer.fit_on_texts(corpus)\n",
    "    \n",
    "    tensor = tokenizer.texts_to_sequences(corpus)\n",
    "    \n",
    "    tensor = [x for x in tensor if len(x) <= 15]\n",
    "    \n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post', maxlen=15)\n",
    "    \n",
    "    print(tensor, tokenizer)\n",
    "    return tensor, tokenizer\n",
    "\n",
    "tensor, tokenizer = tokenize(corpus)\n",
    "\n",
    "src_input = tensor[:, :-1]\n",
    "\n",
    "tgt_input = tensor[:, 1:]\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "enc_train, enc_val, dec_train, dec_val = train_test_split(src_input, tgt_input, test_size=0.2, random_state = 42)\n",
    "\n",
    "BUFFER_SIZE = len(src_input)\n",
    "BATCH_SIZE = 256\n",
    "steps_per_epoch = len(src_input) // BATCH_SIZE\n",
    "\n",
    "VOCAB_SIZE = tokenizer.num_words + 1\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((src_input, tgt_input))\n",
    "dataset = dataset.shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder = True)\n",
    "\n",
    "class TextGenerator(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_size, hidden_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_size)\n",
    "        self.rnn_1 = tf.keras.layers.LSTM(hidden_size, return_sequences = True)\n",
    "        self.rnn_2 = tf.keras.layers.LSTM(hidden_size, return_sequences = True)\n",
    "        self.linear = tf.keras.layers.Dense(vocab_size)\n",
    "        \n",
    "    def call(self, x):\n",
    "        out = self.embedding(x)\n",
    "        out = self.rnn_1(out)\n",
    "        out = self.rnn_2(out)\n",
    "        out = self.linear(out)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "embedding_size = 64\n",
    "hidden_size = 1024\n",
    "model = TextGenerator(tokenizer.num_words + 1, embedding_size, hidden_size)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "\n",
    "model.compile(loss=loss, optimizer = optimizer)\n",
    "model.fit(dataset, epochs = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d54db81",
   "metadata": {},
   "source": [
    "## ...마치며\n",
    "\n",
    "첫 번째로, preprocess_sentence를 이용하여 15 단어 이하인 문장만 사용하라고 되어 있어 문장을 split하여 리스트로 변환 후 숫자를 세어서 16개 이상인 것은 제외하도록 했는데, 두 번째로, 문장을 토크나이즈 하고 텐서에 저장할 때 15개 이하만 저장하도록 하는 방법으로 했을 때와 사용하는 데이터 수가 다르게 되어 첫번째로 사용한 방법이 맞는지 의문이 들지만, 그래도 목표였던 loss값이 2.2 이하로 떨어지게 하는 것에 성공했다.\n",
    "\n",
    "문장을 만드는 것을 보면 같은 초기값을 줘도 출력 값이 다른 것을 확인할 수 있는데, 사용했던 데이터의 수가 달랐기 때문인 것 같다.\n",
    "\n",
    "두 번째 방법을 보면 EPOCHS = 10을 거치며 오답률이 3.4376에서 시작해 2.1440으로 감소하고, 회당 111s가 걸린 것을 알 수 있다.\n",
    "목표는 2.2 아래로 내리는 것이었는데, 충분히 달성한 것 같다.\n",
    "\n",
    "#### embedding_size 와 hidden_size 값에 따른 loss값과 회당 걸리는 시간\n",
    "\n",
    "위의 두 번째 방법에서 임베딩 사이즈와 히든 사이즈를 바꿔가며 실행 해보았다.\n",
    "\n",
    "##### 임베딩 사이즈를 줄인 경우\n",
    "* embedding_size = 128, hidden_size = 1024일 때, Epoch 1/10 - loss: 3.4776, Epoch 10/10 - loss: 2.2420으로 회당 111s의 시간이 걸렸고,\n",
    "\n",
    "* embedding_size = 64, hidden_size = 1024일 때, Epoch 1/10 - loss: 3.5137, Epoch 10/10 - loss: 2.3620으로 회당 110s의 시간이 걸렸다.\n",
    "\n",
    "##### 히든사이즈를 줄인 경우\n",
    "\n",
    "* embedding_size = 256, hidden_size = 512일 때는, Epoch 1/10 - loss: 3.7359, Epoch 10/10 - loss: 2.4145으로 회당50s의 시간이 걸렸고,\n",
    "\n",
    "* embedding_size = 256, hidden_size = 256일 때, Epoch 1/10 - loss: 3.8797, Epoch 10/10 - loss: 2.6682으로으로 회당 27s의 시간이 걸렸으며,\n",
    "\n",
    "* embedding_size = 256, hidden_size = 128일 때는, Epoch 1/10 - loss: 4.1071, Epoch 10/10 - loss: 2.8133으로 회당18s의 시간이 걸렸다.\n",
    "\n",
    "\n",
    "-> 이러한 경향으로 봤을 때, 히든사이즈 값이 작을 수록 loss값이 커지며, 회당 걸리는 시간이 짧아진다. 임베딩 사이즈는 값이 작을 수록 loss값이 커지지만 회당 걸리는 시간은 별 변화가 없는 것을 알 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5cc0e3a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
